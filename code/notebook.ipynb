{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "WIRNotebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiseBtETqfIC",
        "colab_type": "text"
      },
      "source": [
        "# Configuration\n",
        "Install and import services."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TzTZcDFUaEW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "adbe38d1-d8e3-494f-a727-15d15fabe034"
      },
      "source": [
        "# General purpose modules\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "\n",
        "#Read csv files adn plots\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import ast\n",
        "\n",
        "#Retrieval from twitter\n",
        "import tweepy\n",
        "from getpass import getpass\n",
        "\n",
        "#Tweet preprocessing tasks\n",
        "!pip install emoji\n",
        "!pip install pyspellchecker\n",
        "import re\n",
        "import emoji\n",
        "from spellchecker import SpellChecker\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "#random numbers\n",
        "from random import randint\n",
        "\n",
        "# Progress bar utility\n",
        "def initialize_progress_bar():\n",
        "  # setup progressbar\n",
        "  toolbar_width = 40\n",
        "  sys.stdout.write(\"[%s]\" % (\" \" * toolbar_width))\n",
        "  sys.stdout.flush()\n",
        "  sys.stdout.write(\"\\b\" * (toolbar_width+1)) # return to start of line, after '['\n",
        "\n",
        "def manage_progress_bar(j):\n",
        "  if (j == 0): sys.stdout.write(\" \" * 4)\n",
        "  j += 1\n",
        "  n = 5\n",
        "  if (len(str(j*100//40)) < 2): n = 4\n",
        "  sys.stdout.write(\"\\b\" * n)\n",
        "  sys.stdout.write(\"- %s\" % (str(j*100//40)))\n",
        "  sys.stdout.write(\"%]\")\n",
        "  sys.stdout.flush()\n",
        "  if (j == 40): sys.stdout.write(\"\\n\") # end the progress bar\n",
        "  return j\n",
        "\n",
        "#Access to Google drive file system\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sloux1o0d_P",
        "colab_type": "text"
      },
      "source": [
        "## Load files in primary storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzLjPiG9UaEd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99a4bc0d-19ba-4758-976f-2a9bdcde91d3"
      },
      "source": [
        "kbase = pd.read_csv('Datasets/WIR_P12/dbpedia-classes/DBP_wiki_data_notext_nocomma.csv')\n",
        "kbase.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'l1', 'l2', 'l3', 'wiki_name'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alIzxZMoUaEx",
        "colab_type": "text"
      },
      "source": [
        "## Knowledge Base Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhOf1IPxLljE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_occurrences(level):\n",
        "\n",
        "    categories = kbase[level]\n",
        "\n",
        "    category_info_dict = {}\n",
        "\n",
        "    for cat in categories:\n",
        "        if cat in category_info_dict:\n",
        "            category_info_dict[cat] += 1\n",
        "        else:\n",
        "            category_info_dict[cat] = 0\n",
        "    return category_info_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfa9bHv9UaE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "def plot_kb_distr(level):\n",
        "\n",
        "    category_info_dict = count_occurrences(level)\n",
        "\n",
        "    labels = list(category_info_dict.keys())\n",
        "    values = list(category_info_dict.values())\n",
        "\n",
        "    plt.figure(figsize=(20,len(values)))\n",
        "    plt.title('Total length of categories')\n",
        "    \n",
        "    cmap = {'l1': 'r', 'l2':'g', 'l3': 'b'}\n",
        "\n",
        "    for i, cat in enumerate(category_info_dict):\n",
        "        plt.barh(labels, values, color=cmap[level])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    sorted_categories = sorted(category_info_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if level == 'l2':\n",
        "        print('TOP 23 CATEGORIES IN THE KB:')\n",
        "        for cat, value in sorted_categories[:23]:\n",
        "            print('{}: {} pages'.format(cat, value))\n",
        "            \n",
        "# plot_kb_distr('l1')\n",
        "# plot_kb_distr('l2')\n",
        "# plot_kb_distr('l3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml8m_m2rVK9S",
        "colab_type": "text"
      },
      "source": [
        "# Tweets dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3HzNoNzVeIH",
        "colab_type": "text"
      },
      "source": [
        "## Authentication via Twitter App\n",
        "1. Provide the keys of your twitter app.\n",
        "2. Establish an authenticated session with the Standard Twitter API through the tweepy library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Sbqfc4RVUds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# consumer_key = getpass('Enter consumer_key here')\n",
        "# consumer_secret = getpass('Enter consumer_secret here')\n",
        "# access_token = getpass('Enter access_token here')\n",
        "# access_token_secret = getpass('Enter access_token_secret here')\n",
        "\n",
        "# auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "# auth.set_access_token(access_token, access_token_secret)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXxL2TFDVgDi",
        "colab_type": "text"
      },
      "source": [
        "## Collect Tweets\n",
        "Collect tweets through the Twitter session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A79Qt0vVWdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def query_twitter(query):\n",
        "#     api = tweepy.API(auth)\n",
        "#     max_tweets = 5\n",
        "#     public_tweets = [status for status in tweepy.Cursor(api.search, q=query, lang='en', tweet_mode='extended').items(max_tweets)]\n",
        "#     i = 0\n",
        "\n",
        "#     for tweet in public_tweets:\n",
        "#         print(str(i) + ': ' + tweet.full_text)\n",
        "#         dictionary['tweets'].append(tweet.full_text)\n",
        "#         print('-------------------------------------------------------------------')\n",
        "#         i = i+1\n",
        "\n",
        "# #Examples of queries submitted to the twitter API\n",
        "# dictionary = {'tweets':[]}\n",
        "# query_twitter('National Geographic')\n",
        "# query_twitter('People Magazine')\n",
        "# query_twitter('Family Circle')\n",
        "# query_twitter('Game Informer Magazine')\n",
        "\n",
        "# #Create a Pandas dataframe and write it on a .csv file\n",
        "# dataframe = pd.DataFrame(dictionary)\n",
        "# dataframe.to_csv('tweets/periodical_literature.csv')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFgw5c2Gwuxg",
        "colab_type": "text"
      },
      "source": [
        "# Implementation of the system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4CeVKpmoNF-",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess tweet\n",
        "* method to preprocess the tweet:\n",
        "    * input: raw tweet\n",
        "    * output: list of tokens extracted from the raw tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIrwfmtjvrNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tweet_preprocessing(tweet):\n",
        "  #remove RT\n",
        "  tweet = re.sub(r'RT', '', tweet)\n",
        "  #remove urls from the tweet\n",
        "  tweet = re.sub(r'http\\S+', '', tweet)\n",
        "  #remove ats, hashtags and emails\n",
        "  tweet = re.sub(r'[^\\s]+@[^\\s]+', '', tweet)\n",
        "  tweet = re.sub(r'@', '', tweet)\n",
        "  tweet = re.sub(r'#', '', tweet)\n",
        "  #remove emojis, symbols and flags\n",
        "  tweet = emoji.get_emoji_regexp().sub(u'', tweet)\n",
        "  #tokenize tweet\n",
        "  tweet = word_tokenize(tweet)\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGlmJ7Svpww_",
        "colab_type": "text"
      },
      "source": [
        "## Build prefix map\n",
        "* This code snippet creates a prefix map over the entries of the *DBP_wiki_data_notext_nocomma.csv* knowledge base.\n",
        "* The prefix map is kept in main memory under the *prefix_map* variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q2JQeJ5p15A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bebd098d-4d84-4a9a-e05e-6de65459535d"
      },
      "source": [
        "# entity_list is simply a list of wiki_names like \"Cristiano_Ronaldo\"\n",
        "def prefix_map_add_batch(entity_list):\n",
        "  size = len(entity_list)\n",
        "  if (size>40):\n",
        "    initialize_progress_bar()\n",
        "  i_prog = j_prog = 0  # progress bar variables\n",
        "\n",
        "  for entry in entity_list:  # entry i.e. each wiki_name\n",
        "    i_prog += 1\n",
        "    prefix = ''\n",
        "    for token in word_tokenize(entry.replace('_', ' ')):\n",
        "      prefix = (prefix + ' ' + token).strip()  # update prefix\n",
        "      \n",
        "      kb_node = None\n",
        "      stop = False\n",
        "      if prefix == entry.replace('_', ' '):  # if prefix == entry\n",
        "        kb_node = entry  # set the kb_node\n",
        "        stop = True\n",
        "\n",
        "      try:\n",
        "        # check if the prefix is already in the map\n",
        "        found = False\n",
        "        for i in range(len(prefix_map[hash(prefix)])):\n",
        "          # if it is already contained\n",
        "          if prefix == prefix_map[hash(prefix)][i][0]:\n",
        "            found = True\n",
        "            # if we are not done with the current entry, set stop=False for the map entry\n",
        "            # otherwise, leave it unchanged\n",
        "            if not stop:\n",
        "              prefix_map[hash(prefix)][i][2] = False\n",
        "            else:\n",
        "              prefix_map[hash(prefix)][i][1] = kb_node\n",
        "            break\n",
        "        # if it is not, append it\n",
        "        if not found:\n",
        "          prefix_map[hash(prefix)].append([prefix, kb_node, stop])\n",
        "      except KeyError:\n",
        "        prefix_map[hash(prefix)] = [[prefix, kb_node, stop]]\n",
        "    # Managing the progress bar\n",
        "    if (size > 40 and i_prog % (size//40) == 0):\n",
        "      j_prog = manage_progress_bar(j_prog)\n",
        "\n",
        "prefix_map = {}\n",
        "prefix_map_add_batch(kbase['wiki_name'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[                                        ]\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b    \b\b\b\b- 2%]\b\b\b\b- 5%]\b\b\b\b- 7%]\b\b\b\b\b- 10%]\b\b\b\b\b- 12%]\b\b\b\b\b- 15%]\b\b\b\b\b- 17%]\b\b\b\b\b- 20%]\b\b\b\b\b- 22%]\b\b\b\b\b- 25%]\b\b\b\b\b- 27%]\b\b\b\b\b- 30%]\b\b\b\b\b- 32%]\b\b\b\b\b- 35%]\b\b\b\b\b- 37%]\b\b\b\b\b- 40%]\b\b\b\b\b- 42%]\b\b\b\b\b- 45%]\b\b\b\b\b- 47%]\b\b\b\b\b- 50%]\b\b\b\b\b- 52%]\b\b\b\b\b- 55%]\b\b\b\b\b- 57%]\b\b\b\b\b- 60%]\b\b\b\b\b- 62%]\b\b\b\b\b- 65%]\b\b\b\b\b- 67%]\b\b\b\b\b- 70%]\b\b\b\b\b- 72%]\b\b\b\b\b- 75%]\b\b\b\b\b- 77%]\b\b\b\b\b- 80%]\b\b\b\b\b- 82%]\b\b\b\b\b- 85%]\b\b\b\b\b- 87%]\b\b\b\b\b- 90%]\b\b\b\b\b- 92%]\b\b\b\b\b- 95%]\b\b\b\b\b- 97%]\b\b\b\b\b- 100%]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKT8vKbuwMiE",
        "colab_type": "text"
      },
      "source": [
        "## Build the Inverted Index\n",
        "Create an inverted index that, given a *wiki_name*, returns the corresponding lineage [l1,l2,l3]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RreIfR_JXycx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9114164b-10aa-4e48-dc59-e267c53cc578"
      },
      "source": [
        "initialize_progress_bar()\n",
        "\n",
        "inv_index = {}\n",
        "i = j = 0 # parameters for the progress bar\n",
        "for index, row in kbase.iterrows():\n",
        "  i += 1\n",
        "  inv_index[row['wiki_name']] = [row['l1'], row['l2'], row['l3']]\n",
        "\n",
        "  # Managing the progress bar\n",
        "  if (i % (len(kbase)//40) == 0):\n",
        "    j = manage_progress_bar(j)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[                                        ]\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b    \b\b\b\b- 2%]\b\b\b\b- 5%]\b\b\b\b- 7%]\b\b\b\b\b- 10%]\b\b\b\b\b- 12%]\b\b\b\b\b- 15%]\b\b\b\b\b- 17%]\b\b\b\b\b- 20%]\b\b\b\b\b- 22%]\b\b\b\b\b- 25%]\b\b\b\b\b- 27%]\b\b\b\b\b- 30%]\b\b\b\b\b- 32%]\b\b\b\b\b- 35%]\b\b\b\b\b- 37%]\b\b\b\b\b- 40%]\b\b\b\b\b- 42%]\b\b\b\b\b- 45%]\b\b\b\b\b- 47%]\b\b\b\b\b- 50%]\b\b\b\b\b- 52%]\b\b\b\b\b- 55%]\b\b\b\b\b- 57%]\b\b\b\b\b- 60%]\b\b\b\b\b- 62%]\b\b\b\b\b- 65%]\b\b\b\b\b- 67%]\b\b\b\b\b- 70%]\b\b\b\b\b- 72%]\b\b\b\b\b- 75%]\b\b\b\b\b- 77%]\b\b\b\b\b- 80%]\b\b\b\b\b- 82%]\b\b\b\b\b- 85%]\b\b\b\b\b- 87%]\b\b\b\b\b- 90%]\b\b\b\b\b- 92%]\b\b\b\b\b- 95%]\b\b\b\b\b- 97%]\b\b\b\b\b- 100%]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA0bKyC9FyTS",
        "colab_type": "text"
      },
      "source": [
        "## Extract Mentions\n",
        "* This cell contains a function that has the role to search formentions in a given list of tweets\n",
        "* This search exploits the prefix map, in order to search efficiently in the knowledge base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxWf3kfVF5BA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_mentions(tweets):\n",
        "  tweets_mentions = []\n",
        "  #scan all the tweets in the dataset\n",
        "  for tokens in tweets:\n",
        "    #list of mentions for the current tweet\n",
        "    mentions = []\n",
        "    for i in range(len(tokens)):\n",
        "      token = tokens[i]\n",
        "      #if the token is in the kb\n",
        "      try:\n",
        "        #True if you have found a mention\n",
        "        done = False\n",
        "        #scan consecutive tokens in the tweet\n",
        "        j = 1;\n",
        "        while (i+j) <= len(tokens):\n",
        "          #Keep track of the last kb node found\n",
        "          last_kb_node = None\n",
        "          #compute the hash for the selected tokens\n",
        "          prefix_map_list = prefix_map[hash(token)]\n",
        "          #scan the adjacency list of the HashMap\n",
        "          for prefix_map_token in prefix_map_list:\n",
        "            #take the entry that you need from the adjacency list\n",
        "            if prefix_map_token[0].lower() == token.lower():\n",
        "              #stop is true, return what you have\n",
        "              if prefix_map_token[2] == True:\n",
        "                last_kb_node = prefix_map_token[1]\n",
        "                mentions.append(last_kb_node)\n",
        "                done = True\n",
        "              #stop is false\n",
        "              else:\n",
        "                if prefix_map_token[1] != None:\n",
        "                    last_kb_node = prefix_map_token[1]\n",
        "                if i+j != len(tokens):\n",
        "                  token = token + ' ' + tokens[i+j]\n",
        "                  j += 1\n",
        "                else:\n",
        "                  j += 1\n",
        "          #Found a mention\n",
        "          if done == True:\n",
        "              next_token = True\n",
        "              break\n",
        "\n",
        "        if last_kb_node != None:\n",
        "            mentions.append(last_kb_node)\n",
        "      except KeyError:\n",
        "          pass\n",
        "    #Add the final mentions for this tweet to a global list\n",
        "    tweets_mentions.append(mentions)\n",
        "  return tweets_mentions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nekjT3fWIEnT",
        "colab_type": "text"
      },
      "source": [
        "## Classifying and Tagging the Tweet\n",
        "Dopo aver estratto le mentions di un tweet dalla prefix map, posso procedere alla classificazione.\n",
        "\n",
        "```\n",
        "# INPUT(mentions) // we refer to the mentions of the tweet to be classified\n",
        "# OUTPUT(class_l2)\n",
        "```\n",
        "\n",
        "For each mention (m1, n1, s1):\n",
        "* Starting at n1, we go up all lineages of n1, all the way to the root, and assign to each node in these lineages a score (currently set to be s1, the same score as that of n1).\n",
        "* If we find two partially overlapping lineages, the common nodes in the *kbase* should get an increased score. Start with s1+s2, later on we can refine this score.\n",
        "* Let C be the decreasingly ordered list of all scored nodes in all lineages. **Select from C all topic nodes**. Starting from C we can both produce a classification (pick the highest) and a tagging (pick all).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jioE-UH3l6Uw",
        "colab_type": "text"
      },
      "source": [
        "## Taxonomic score (struct)\n",
        "Produce a scoring data structure with:\n",
        "*   the shape of the taxonomic hierarchy\n",
        "*   an additional field **score**, as a placeholder of the score of that category.\n",
        "```\n",
        "## INPUT (./dbpedia-classes/taxonomy.txt)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0saatm-kkwp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_tax_score():\n",
        "  tax_file = open(\"Datasets/WIR_P12/dbpedia-classes/taxonomy.txt\", \"r\")\n",
        "  tax_score = {}\n",
        "\n",
        "  for line in tax_file:\n",
        "    if (not line[0]=='\\t'):\n",
        "      l1 = word_tokenize(line)[0]\n",
        "      tax_score[l1] = {}\n",
        "      tax_score[l1]['score'] = 0\n",
        "    else:\n",
        "      if (not line[1]=='\\t'):\n",
        "        l2 = word_tokenize(line)[0]\n",
        "        tax_score[l1][l2] = {}\n",
        "        tax_score[l1][l2]['score'] = 0\n",
        "      else:\n",
        "        l3 = word_tokenize(line)[0]\n",
        "        tax_score[l1][l2][l3] = {}\n",
        "        tax_score[l1][l2][l3]['score'] = 0\n",
        "  return tax_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERBwK8kHkfJK",
        "colab_type": "text"
      },
      "source": [
        "## Classifying and Tagging\n",
        "* Assign the scores on the lineages of the detected mentions.\n",
        "* I shall return the L2 categories in *score decreasing order*. Moreover, I can return the L1, L3 categories (when existing) as tags of the tweet.\n",
        "\n",
        "```\n",
        "# INPUT(mentions)\n",
        "# OUTPUT(categories, tags)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjHblrWtrHal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use only within the scope of classify_and_tag\n",
        "\n",
        "# Returns only the top scores categories from a list of \n",
        "# all categories sorted by score (reversed).\n",
        "def filter_classes(alist):\n",
        "  if (alist[0][1]==0):\n",
        "    return []\n",
        "  \n",
        "  top_list = []\n",
        "  top_list.append(alist[0][0])\n",
        "  max = alist.pop(0)[1]\n",
        "  for i in alist:\n",
        "    if (i[1]==max):\n",
        "      top_list.append(i[0])\n",
        "    else:\n",
        "      break\n",
        "  \n",
        "  return top_list\n",
        "\n",
        "# Returns only non-zero tags.\n",
        "def filter_tags(alist):\n",
        "  ret_list = []\n",
        "\n",
        "  for i in alist:\n",
        "    if (not i[1]==0):\n",
        "      ret_list.append(i[0])\n",
        "    else:\n",
        "      break\n",
        "  \n",
        "  return ret_list\n",
        "\n",
        "def classify_and_tag(tweet_mention): # the mentions of a single tweet\n",
        "  score = create_tax_score()\n",
        "  categories = []\n",
        "  tags = []\n",
        "  for mention in tweet_mention:\n",
        "    lineage = inv_index[mention]\n",
        "    l1 = lineage[0]\n",
        "    l2 = lineage[1]\n",
        "    l3 = lineage[2]\n",
        "    score[l1]['score'] += 1\n",
        "    score[l1][l2]['score'] += 1\n",
        "    if (not l3 == 'none'):\n",
        "      score[l1][l2][l3]['score'] += 1\n",
        "  for x in score.keys():\n",
        "    tags.append((x,score[x]['score']))\n",
        "    for y in score[x].keys():\n",
        "      if(y == 'score'): continue\n",
        "      categories.append((y,score[x][y]['score']))\n",
        "      tags.append((y,score[x][y]['score']))\n",
        "      for z in score[x][y].keys():\n",
        "        if(z == 'score'): continue\n",
        "        tags.append((z,score[x][y][z]['score']))\n",
        "  categories.sort(key=lambda x : x[1], reverse=True)\n",
        "  tags.sort(key=lambda x : x[1], reverse=True)\n",
        "  categories = filter_classes(categories)\n",
        "  tags = filter_tags(tags)\n",
        "\n",
        "  return (categories, tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKGCCEoSYCvc",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caeRimzuYHEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function that comutes precision, recall and f1 score\n",
        "def evaluation_report(pred, gt):\n",
        "  tp = 0\n",
        "  fp = 0\n",
        "  fn = 0\n",
        "\n",
        "  for i in range(len(pred)):\n",
        "    #build true positives, false positives, and false negatives counters\n",
        "    for prediction in pred[i]:\n",
        "      if prediction in gt[i]:\n",
        "        tp += 1\n",
        "      else:\n",
        "        fp += 1\n",
        "    for ground_truth in gt[i]:\n",
        "      if ground_truth not in pred[i]:\n",
        "          fn += 1\n",
        "  \n",
        "  #compute precision, recall, and f1 score\n",
        "  if tp != 0:\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    accuracy = tp / len(pred)\n",
        "  else:\n",
        "    return 0,0,0,0\n",
        "  return precision, recall, f1, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc2nGsMU81Ox",
        "colab_type": "text"
      },
      "source": [
        "## Update of the knowledge base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKQmx2VQ86vF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Read the json file to memorize the taxonomy of the kb\n",
        "json_file = open('Datasets/WIR_P12/dbpedia-classes/taxonomy.json',)\n",
        "taxonomy = json.load(json_file)\n",
        "\n",
        "def search_prefix_map(token):\n",
        "    try:\n",
        "        token = token.replace('_', ' ')\n",
        "        #compute the hash for the selected tokens\n",
        "        prefix_map_list = prefix_map[hash(token)]\n",
        "        #scan the adjacency list of the HashMap\n",
        "        for prefix_map_token in prefix_map_list:\n",
        "            #take the entry that you need from the adjacency list\n",
        "            if prefix_map_token[0].lower() == token.lower():\n",
        "                return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# gt: List<List<Pair<Str,Str>>>, where \n",
        "# for_each p in l in gp | p = \"Cristiano_Ronaldo\", \"Athlete\", \n",
        "# I.E.: <\"Sample\", \"L2\">\n",
        "# Note: gp is the list of mentions (one for each tweet)\n",
        "def update_kb(gt):\n",
        "    global kbase\n",
        "\n",
        "    #create a unique dictionary from the groundtruth list\n",
        "    unique_gt = {}\n",
        "    for item in gt:\n",
        "        unique_gt.update(item)\n",
        "\n",
        "    for entity,category in unique_gt.items():\n",
        "        #check if the term is contained in the prefix_map and therefore in the kb\n",
        "        contained = search_prefix_map(entity)\n",
        "        if contained == False:\n",
        "            #take the l1 category from the current l2\n",
        "            root_category = taxonomy[category]\n",
        "            #append the new mention to the updated kb\n",
        "            kbase = kbase.append({'l1':root_category, 'l2':category, 'wiki_name':entity}, ignore_index=True)\n",
        "            prefix_map_add_batch([entity])\n",
        "            inv_index[entity] = [root_category, category, 'none']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STxUhf4TxzTP",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation of Mentions Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X806xWgazZXt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ea5ef655-9354-4b69-8529-ff1abf9995fc"
      },
      "source": [
        "#every data file has the name of the corresponding l2 category\n",
        "categories = []\n",
        "\n",
        "for filename in os.listdir('Datasets/WIR_P12/tweets/'):\n",
        "    categories.append(filename[:-4])\n",
        "\n",
        "pred = []\n",
        "gt_mentions = []\n",
        "\n",
        "for category in categories:\n",
        "    gt = []  # placeholder for the ground truth mentions for each tweet\n",
        "    \n",
        "    tweets = pd.read_csv('Datasets/WIR_P12/tweets/'+ category +'.csv')\n",
        "    #separate the ground truth from the tweet text\n",
        "    for mentions in tweets.mentions:\n",
        "        mentions = mentions.replace('\\'', '\"')\n",
        "        gt.append(json.loads(mentions))\n",
        "\n",
        "    #preprocess and extract mentions from tweets\n",
        "    for i in range(len(tweets.tweets)):\n",
        "        tweets.tweets[i] = tweet_preprocessing(tweets.tweets[i])\n",
        "    pred = pred + (extract_mentions(tweets.tweets))\n",
        "\n",
        "    #print the predictions and the ground truth\n",
        "    for i in range(len(tweets.tweets)):\n",
        "        gt_mentions.append(list(dict.fromkeys(gt[i])))\n",
        "\n",
        "    #Update the Knowledge base with the new groundtruth mentions\n",
        "    #Comment this line if, ater the evaluation of the mentions extraction, you want to evaluate unupdated classification\n",
        "    update_kb(gt)\n",
        "\n",
        "#print the evaluation scores\n",
        "precision, recall, f1, accuracy = evaluation_report(pred[2:], gt_mentions[2:])\n",
        "print('Precision: {}, Recall: {}, F1_score: {}'.format(precision, recall, f1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.9133858267716536, Recall: 0.5144124168514412, F1_score: 0.6581560283687944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVabDUeWKGjC",
        "colab_type": "text"
      },
      "source": [
        "## Random Distribution\n",
        "\n",
        "* this cell defines a function that makes a prediction on the category of a tweet\n",
        "* This prediction is based on a probability distribution taken from the topology of our knowledge base\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obGGz3PHKPec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def probabilistic_prediction(check = False, categories = kbase['l2']):\n",
        "    #this is the final prediction on the probability distribution of the kb\n",
        "    pred = None\n",
        "    categories = list(categories)\n",
        "    #we generate a random number from 1 to 100\n",
        "    prediction_number = randint(1,100)\n",
        "\n",
        "    #count the total number of categories in the kb\n",
        "    total_categories = 0\n",
        "    categories_counter = count_occurrences('l2')\n",
        "    for k,occurrences in categories_counter.items():\n",
        "        #we do this check in order to not search in the entire kb when categories == kbase['l2']\n",
        "        if check == True:\n",
        "            #if check is true, this means that we will search in a small list of categories\n",
        "            if k in categories:\n",
        "                total_categories += occurrences\n",
        "        else:\n",
        "            total_categories += occurrences\n",
        "    category_percentage = {}\n",
        "\n",
        "    #compute the probabilities with a similar method as above\n",
        "    for k,occurrences in categories_counter.items():\n",
        "        if check == True:\n",
        "            if k in categories:\n",
        "                category_percentage[k] = (occurrences / total_categories) * 100\n",
        "        else:\n",
        "            category_percentage[k] = (occurrences / total_categories) * 100\n",
        "\n",
        "    #create tresholds to predict the result\n",
        "    incremental_percentage = 0\n",
        "    for k,percentage in category_percentage.items():\n",
        "        incremental_percentage += percentage\n",
        "        if prediction_number <= incremental_percentage:\n",
        "            pred = k\n",
        "            break\n",
        "\n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-At212HvbEI",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation of Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAql6zBlvlDK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fd4977e-dfb6-4ec9-880d-3fa282bcc7a9"
      },
      "source": [
        "def classification_evaluation():\n",
        "    gt = []\n",
        "\n",
        "    for category in categories:\n",
        "        #Read_tweets from the dataset\n",
        "        tweets = pd.read_csv('Datasets/WIR_P12/tweets/'+ category +'.csv')\n",
        "        for i in range(len(tweets)):\n",
        "            gt.append([category])\n",
        "\n",
        "    classes = []\n",
        "    tags = []\n",
        "    for i in range(len(pred)):\n",
        "        x,y = classify_and_tag(pred[i])\n",
        "        classes.append(x)\n",
        "        tags.append(y)\n",
        "\n",
        "        #If nothing found predict something according to the knowledge base distribution\n",
        "        if classes[i] == []:\n",
        "            classes[i].append(probabilistic_prediction())\n",
        "\n",
        "        #If we have classes with the same score, choose one of them according to the knowledge base distribution\n",
        "        if len(classes[i]) > 1:\n",
        "            classes[i] = [probabilistic_prediction(True, classes[i])]\n",
        "\n",
        "    precision, recall, f1, accuracy = evaluation_report(classes,gt)\n",
        "    return precision, recall, f1, accuracy\n",
        "\n",
        "\n",
        "#When our system is undecided makes a probabilistic classification, so for evaluation purposes we repeat\n",
        "#the classification task 100 times and get the arithmetic mean of precision,recall,f1_score and accuracy\n",
        "precision, recall, f1, accuracy = 0, 0, 0, 0\n",
        "for i in range(100):\n",
        "    prec, rec, f, acc = classification_evaluation()\n",
        "    precision += prec\n",
        "    recall += rec\n",
        "    f1 += f\n",
        "    accuracy += acc\n",
        "precision = precision / 100\n",
        "recall = recall / 100\n",
        "f1 = f1 / 100\n",
        "accuracy = accuracy / 100\n",
        "\n",
        "print('Precision: {}, Recall: {}, F1_score: {}, Accuracy: {}'.format(precision, recall, f1, accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.9133858267716536, Recall: 0.5144124168514412, F1_score: 0.6581560283687944, Accuracy: 1.2747252747252746\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}